# feature_dir: "/data/rbg/shared/datasets/NLST/NLST/Pathology/features/hoptimus1"
#feature_dir: "/data/rbg/shared/datasets/TCGA/Pathology/trident/features_hoptimus1"
#feature_dir: "/storage/silterra/features_hoptimus1"
#feature_dir: "/storage/azizayed/features_uni_v2"
#feature_dir: "/data/rbg/users/azizayed/chemistry/trident/outputs_tcga/20x_256px_0px_overlap/features_uni_v2"
feature_dim: 57
aggregator: "MLP"       # MeanPool | MaxPool | AttnMIL | transmil | Passthrough | MLP | AttnMILNew
batch_size: 128                # slides per GPU
seeds:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
limit_val_batches: null
mini_batches: 1         # Split each batch into `mini_batches` before calculating loss. Used to save memory for large models.
epochs: 100
learning_rate: 5e-5
weight_decay: 1e-5
topk_corr: 256               # for TransMIL
save_checkpoints: true
save_dir: "results/tcga/clinical_js/{aggregator}/{seed}"
gpus: [0,1,2,3,4,5,6]
num_workers: 4
clinical_csv: "data/clinical_model_features.csv"
wandb:
  project: tcga_survival_baselines
  run_name: "{aggregator}_clinical_all_seed_{seed}"
  mode: online          # "online" | "offline" | "disabled"
  log_grads: true      # true = wandb.watch(model) on rank 0
log_every: 50
prefetch_factor: 4
persistent_workers: true
split_file: "data/clean_splits.csv"
eval:
  limit: null
  split: test # train | val | test
  score_path: "results/tcga/clinical_js/{aggregator}/{seed}/clinical_evaluation_scores_{split}.csv"
  plots_path: "results/tcga/clinical_js/{aggregator}/{seed}/clinical_evaluation_plots_{split}.pdf"
  overwrite: true
